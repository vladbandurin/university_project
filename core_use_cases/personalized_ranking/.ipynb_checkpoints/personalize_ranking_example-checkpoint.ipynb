{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalize Ranking Example <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "In this notebook, you will choose a dataset and prepare it for use with Amazon Personalize Batch Recommendations.\n",
    "\n",
    "1. [Choose a dataset or data source](#source)\n",
    "1. [Prepare your data](#prepare)\n",
    "1. [Create dataset groups and the interactions dataset](#group_dataset)\n",
    "1. [Configure an S3 bucket and an IAM role](#bucket_role)\n",
    "1. [Import the interactions data](#import)\n",
    "1. [Create solutions](#solutions)\n",
    "1. [Create campaigns](#create)\n",
    "1. [Interact with campaigns](#interact)\n",
    "1. [Clean up](#cleanup)\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "For the most part, the algorithms in Amazon Personalize (called recipes) look to solve different tasks, explained here:\n",
    "\n",
    "1. **HRNN & HRNN-Metadata** - Recommends items based on previous user interactions with items.\n",
    "1. **HRNN-Coldstart** - Recommends new items for which interaction data is not yet available.\n",
    "1. **Personalized-Ranking** - Takes a collection of items and then orders them in probable order of interest using an HRNN-like approach.\n",
    "1. **SIMS (Similar Items)** - Given one item, recommends other items also interacted with by users.\n",
    "1. **Popularity-Count** - Recommends the most popular items, if HRNN or HRNN-Metadata do not have an answer - this is returned by default.\n",
    "\n",
    "No matter the use case, the algorithms all share a base of learning on user-item-interaction data which is defined by 3 core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "1. **ItemID** - The item the user interacted with\n",
    "1. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "We also support event types and event values defined by:\n",
    "\n",
    "1. **Event Type** - Categorical label of an event (browse, purchased, rated, etc).\n",
    "1. **Event Value** - A value corresponding to the event type that occurred. Generally speaking, we look for normalized values between 0 and 1 over the event types. For example, if there are three phases to complete a transaction (clicked, added-to-cart, and purchased), then there would be an event_value for each phase as 0.33, 0.66, and 1.0 respectfully.\n",
    "\n",
    "The event type and event value fields are additional data which can be used to filter the data sent for training the personalization model. In this particular exercise we will not have an event type or event value. \n",
    "\n",
    "## Choose a dataset or data source <a class=\"anchor\" id=\"source\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "As we mentioned, the user-item-iteraction data is key for getting started with the service. This means we need to look for use cases that generate that kind of data, a few common examples are:\n",
    "\n",
    "1. Video-on-demand applications\n",
    "1. E-commerce platforms\n",
    "1. Social media aggregators / platforms\n",
    "\n",
    "There are a few guidelines for scoping a problem suitable for Personalize. We recommend the values below as a starting point, although the [official limits](https://docs.aws.amazon.com/personalize/latest/dg/limits.html) lie a little lower.\n",
    "\n",
    "* Authenticated users\n",
    "* At least 50 unique users\n",
    "* At least 100 unique items\n",
    "* At least 2 dozen interactions for each user \n",
    "\n",
    "Most of the time this is easily attainable, and if you are low in one category, you can often make up for it by having a larger number in another category.\n",
    "\n",
    "Generally speaking your data will not arrive in a perfect form for Personalize, and will take some modification to be structured correctly. This notebook looks to guide you through all of that. \n",
    "\n",
    "To begin with, we are going to use the [Last.FM](https://grouplens.org/datasets/hetrec-2011/) dataset. These are records of the music listening behavior of its users. The data fits our guidelines with a large number for users, items, and interactions.\n",
    "\n",
    "First, you will download the dataset and unzip it in a new folder using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-26 20:45:51--  http://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2589075 (2.5M) [application/zip]\n",
      "Saving to: ‘hetrec2011-lastfm-2k.zip’\n",
      "\n",
      "hetrec2011-lastfm-2 100%[===================>]   2.47M  10.2MB/s    in 0.2s    \n",
      "\n",
      "2020-05-26 20:45:51 (10.2 MB/s) - ‘hetrec2011-lastfm-2k.zip’ saved [2589075/2589075]\n",
      "\n",
      "Archive:  hetrec2011-lastfm-2k.zip\n",
      "  inflating: user_friends.dat        \n",
      "  inflating: user_taggedartists.dat  \n",
      "  inflating: user_taggedartists-timestamps.dat  \n",
      "  inflating: artists.dat             \n",
      "  inflating: readme.txt              \n",
      "  inflating: tags.dat                \n",
      "  inflating: user_artists.dat        \n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "!mkdir $data_dir\n",
    "!cd $data_dir && wget http://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip\n",
    "!cd $data_dir && unzip hetrec2011-lastfm-2k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data files you have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artists.dat\t\t  tags.dat\t    user_taggedartists.dat\n",
      "hetrec2011-lastfm-2k.zip  user_artists.dat  user_taggedartists-timestamps.dat\n",
      "readme.txt\t\t  user_friends.dat\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present not much is known about the data other than we seem to have many .dat files and a README. Opening the README will tell us about the overall structure of this data. This is a step you probably can skip with custom data unless the data source is coming from an external team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the README, we can see that there are multiple interaction types in this dataset. Interactions between users marking each other as friends, interactions from users listening to artists, and interactions from tags assigned to users and artists.\n",
    "\n",
    "In this case, we are focusing on the users, the artists, and the listening interactions. We have 1892 users, 17632 artists (our items in this case), and 92834 user-listened artist interactions. This is more than enough for us to get started with Personalize.\n",
    "\n",
    "Continue reading through the README to get to the `Files` section. Most of the files in the dataset are not relevant to us, but the `users_artists.dat` file looks promising. The `Data format` section of the README provides more details on the contents of the file. This is where we encounter our first problem.\n",
    "\n",
    "| userID | artistID | weight  |\n",
    "|--------|----------|---------|\n",
    "| 2      | 51       | 13883   |\n",
    "\n",
    "Although there is interaction data between users and the artists they are listening to, these interactions are stored as weights instead of timestamps. We need user-item-timestamp interaction data for Amazon Personalize. \n",
    "\n",
    "If you take another look at the files in the dataset, you should see that `users_taggedartists-timestamps.dat` does contain timestamp data. So what if we use tagging behavior as our interaction data, instead of listening behavior? Can we assume that a user tagging an artist is an indication of positive sentiment? Normally, you would discuss with your customer, or someone who has domain knowledge, to understand if this interaction is suitable for the use case you want to solve. For now, we will assume that tagging behavior is suitable for our needs. \n",
    "\n",
    "The schema for the `user_taggedartists-timestamps.dat` is:\n",
    "\n",
    "| userID | artistID | tagID | timestamp     |\n",
    "|--------|----------|-------|---------------|\n",
    "| 2      | 52       | 13    | 1238536800000 |\n",
    "\n",
    "If we remove the `tagID` attribute, we have exactly the format we need for Amazon Personalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your data <a class=\"anchor\" id=\"prepare\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The next thing to be done is to load the data and confirm the data is in a good state, then save it to a CSV where it is ready to be used with Amazon Personalize.\n",
    "\n",
    "To get started, import a collection of Python libraries commonly used in data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next,open the data file and take a look at the first several rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID\tartistID\ttagID\ttimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2\\t52\\t13\\t1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2\\t52\\t15\\t1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2\\t52\\t18\\t1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2\\t52\\t21\\t1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2\\t52\\t41\\t1238536800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userID\\tartistID\\ttagID\\ttimestamp\n",
       "0           2\\t52\\t13\\t1238536800000\n",
       "1           2\\t52\\t15\\t1238536800000\n",
       "2           2\\t52\\t18\\t1238536800000\n",
       "3           2\\t52\\t21\\t1238536800000\n",
       "4           2\\t52\\t41\\t1238536800000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data = pd.read_csv(data_dir + '/user_taggedartists-timestamps.dat')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the data did not load correctly. The default delimiter for CSV (comma-separated value) files is a comma (`,`), but in this case the file was saved with tab (`\\t`) delimiters. So let's specify the correct delimiter and try loading the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>tagID</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>1238536800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>1238536800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID  tagID      timestamp\n",
       "0       2        52     13  1238536800000\n",
       "1       2        52     15  1238536800000\n",
       "2       2        52     18  1238536800000\n",
       "3       2        52     21  1238536800000\n",
       "4       2        52     41  1238536800000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data = pd.read_csv(data_dir + '/user_taggedartists-timestamps.dat', delimiter='\\t')\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. Now that the data has been successfully loaded into memory, let's extract some additional information. First, calculate some basic statistics from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>tagID</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>186479.000000</td>\n",
       "      <td>186479.000000</td>\n",
       "      <td>186479.000000</td>\n",
       "      <td>1.864790e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1035.600137</td>\n",
       "      <td>4375.845328</td>\n",
       "      <td>1439.582913</td>\n",
       "      <td>1.239204e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>622.461272</td>\n",
       "      <td>4897.789595</td>\n",
       "      <td>2775.340279</td>\n",
       "      <td>4.299091e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.287204e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>488.000000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.209593e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1021.000000</td>\n",
       "      <td>2203.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>1.243807e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1624.000000</td>\n",
       "      <td>6714.000000</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>1.275343e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2100.000000</td>\n",
       "      <td>18744.000000</td>\n",
       "      <td>12647.000000</td>\n",
       "      <td>1.304941e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userID       artistID          tagID     timestamp\n",
       "count  186479.000000  186479.000000  186479.000000  1.864790e+05\n",
       "mean     1035.600137    4375.845328    1439.582913  1.239204e+12\n",
       "std       622.461272    4897.789595    2775.340279  4.299091e+10\n",
       "min         2.000000       1.000000       1.000000 -4.287204e+11\n",
       "25%       488.000000     686.000000      79.000000  1.209593e+12\n",
       "50%      1021.000000    2203.000000     195.000000  1.243807e+12\n",
       "75%      1624.000000    6714.000000     887.000000  1.275343e+12\n",
       "max      2100.000000   18744.000000   12647.000000  1.304941e+12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have a good range of values for `userID` and `artistID`. Next, it is always a good idea to confirm the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186479 entries, 0 to 186478\n",
      "Data columns (total 4 columns):\n",
      "userID       186479 non-null int64\n",
      "artistID     186479 non-null int64\n",
      "tagID        186479 non-null int64\n",
      "timestamp    186479 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "original_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can see that there are a total of 186,479 entries in the dataset, with 4 columns, and each cell stored as int64 format.\n",
    "\n",
    "The int64 format is clearly suitable for `userID` and `artistID`. However, we need to diver deeper to understand the timestamps in the data. To use Amazon Personalize, you need to save timestamps in [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time) format.\n",
    "\n",
    "Currently, the timestamp values are not human-readable. So let's grab an arbitrary timestamp value and figure out how to interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235862000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "year 41132 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d2aefeb3727a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marb_time_stamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marb_time_stamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marb_time_stamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: year 41132 is out of range"
     ]
    }
   ],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! For this particular timestamp value, the code rendered a year of 41,132. That's a bit far into the future for us, so clearly this was not the correct way to parse the data. We need a second attempt.\n",
    "\n",
    "JavaScript records time in milliseconds and this is a collection of data from a web application, so let's divide the timestamp value by 1000 before applying our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-02-28 23:00:00\n"
     ]
    }
   ],
   "source": [
    "arb_time_stamp = arb_time_stamp/1000\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "February, 2009 feels much more realistic for our dataset. We don't need human-readable timestamps to use Amazon Personalize, but we do want the dates to be realistic, so now move forward by transforming each timestamp in the dataset away from the JavaScript milliseconds format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>tagID</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID  tagID     timestamp\n",
       "0       2        52     13  1.238537e+09\n",
       "1       2        52     15  1.238537e+09\n",
       "2       2        52     18  1.238537e+09\n",
       "3       2        52     21  1.238537e+09\n",
       "4       2        52     41  1.238537e+09"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.timestamp = original_data.timestamp / 1000\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick sanity check on the transformed dataset by picking an arbitrary timestamp and transforming it to a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235862000.0\n",
      "2009-02-28 23:00:00\n"
     ]
    }
   ],
   "source": [
    "arb_time_stamp = original_data.iloc[50]['timestamp']\n",
    "print(arb_time_stamp)\n",
    "print(datetime.utcfromtimestamp(arb_time_stamp).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This date makes sense as a timestamp, so we can continue formatting the rest of the data. Remember, the data we need is user-item-interaction data, which is `userID`, `artistID`, and `timestamp` in this case. Our dataset has an additional column, `tagID`, which can be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1.238537e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID     timestamp\n",
       "0       2        52  1.238537e+09\n",
       "1       2        52  1.238537e+09\n",
       "2       2        52  1.238537e+09\n",
       "3       2        52  1.238537e+09\n",
       "4       2        52  1.238537e+09"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = original_data.copy()\n",
    "interactions_df = interactions_df[['userID', 'artistID', 'timestamp']]\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manipulating the data, always confirm if the data format has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userID         int64\n",
       "artistID       int64\n",
       "timestamp    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the timestamp column has changed from int64 to float64. So let's change the format back to int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userID       int64\n",
       "artistID     int64\n",
       "timestamp    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.astype({'timestamp': 'int64'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Amazon Personalize has default column names for users, items, and timestamp. These default column names are `USER_ID`, `ITEM_ID`, AND `TIMESTAMP`. So the final modification to the dataset is to replace the existing column headers with the default headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.rename(columns = {'userID':'USER_ID', 'artistID':'ITEM_ID', \n",
    "                              'timestamp':'TIMESTAMP'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! At this point the data is ready to go, and we just need to save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = \"interactions.csv\"\n",
    "interactions_df.to_csv((data_dir+\"/\"+interactions_filename), index=False, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset groups and the interactions dataset <a class=\"anchor\" id=\"group_dataset\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one - they are completely isolated. This allows you to run many experiments and is part of how we keep your models private and fully trained only on your data. \n",
    "\n",
    "Before importing the data prepared earlier, there needs to be a dataset group and a dataset added to it that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "Before we create the dataset group and the dataset for our interaction data, let's validate that your environment can communicate successfully with Amazon Personalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset group\n",
    "\n",
    "The following cell will create a new dataset group with the name `personalize-poc-lastfm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:144386903708:dataset-group/personalize-ranking-dsg\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"386224ac-8410-4f64-ba4a-460ba12607c7\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 20:46:27 GMT\",\n",
      "      \"x-amzn-requestid\": \"386224ac-8410-4f64-ba4a-460ba12607c7\",\n",
      "      \"content-length\": \"102\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"personalize-ranking-dsg\"\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset group, it must be active. This can take a minute or two. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the dataset group every second, up to a maximum of 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataset group, you can create a dataset for the interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. There are several reserved and mandatory keywords required in the schema, based on the type of dataset. More detailed information can be found in the [documentation](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html).\n",
    "\n",
    "Here, you will create a schema for interactions data, which needs the `USER_ID`, `ITEM_ID`, and `TIMESTAMP` fields. These must be defined in the same order in the schema as they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:144386903708:schema/personalize-ranking-interactions\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"d3fb42c4-9a3c-45a4-a842-adf60ef03906\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 20:47:58 GMT\",\n",
      "      \"x-amzn-requestid\": \"d3fb42c4-9a3c-45a4-a842-adf60ef03906\",\n",
      "      \"content-length\": \"98\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"personalize-ranking-interactions\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a schema created, you can create a dataset within the dataset group. Note, this does not load the data yet. This will happen a few steps later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:144386903708:dataset/personalize-ranking-dsg/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"98a3c225-31aa-49be-936f-f633aa6314d0\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 20:48:02 GMT\",\n",
      "      \"x-amzn-requestid\": \"98a3c225-31aa-49be-936f-f633aa6314d0\",\n",
      "      \"content-length\": \"104\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"personalize-ranking-ds\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM  role <a class=\"anchor\" id=\"bucket_role\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to instance running this Jupyter notebook. However, Amazon Personalize will need an S3 bucket to act as the source of your data, as well as IAM roles for accessing that bucket. Let's set all of that up.\n",
    "\n",
    "Use the metadata stored on the instance underlying this Amazon SageMaker notebook, to determine the region it is operating in. If you are using a Jupyter notebook outside of Amazon SageMaker, simply define the region as a string below. The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources we have been creating so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon S3 bucket names are globally unique. To create a unique bucket name, the code below will append the string `personalizepoc` to your AWS account number. Then it creates a bucket with this name in the region discovered in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personalize-ranking-demo-41600\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "suffix = str(np.random.uniform())[4:9]\n",
    "bucket_name = \"personalize-ranking-demo-\"+   suffix        # replace with the name of your S3 bucket\n",
    "print(bucket_name)\n",
    "if region != \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "Now that your Amazon S3 bucket has been created, upload the CSV file of our user-item-interaction data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = data_dir + \"/\" + interactions_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_filename).upload_file(interactions_file_path)\n",
    "interactions_s3DataPath = \"s3://\"+bucket_name+\"/\"+interactions_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the S3 bucket policy\n",
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. So add a bucket policy which allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '61279AD0FB422CD2',\n",
       "  'HostId': 'pvAc3tsFJjLUU5BTudF9gokRma/yRh+uH3TdrVXk3/xoLlHsOev2XPv8KDc+TusZphyzL/TDUvE=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'pvAc3tsFJjLUU5BTudF9gokRma/yRh+uH3TdrVXk3/xoLlHsOev2XPv8KDc+TusZphyzL/TDUvE=',\n",
       "   'x-amz-request-id': '61279AD0FB422CD2',\n",
       "   'date': 'Tue, 26 May 2020 20:48:18 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Amazon Personalize needs the ability to assume roles in AWS in order to have the permissions to execute certain tasks. Let's create an IAM role and attach the required policies to it. The code below attaches very permissive policies; please use more restrictive policies for any production application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::144386903708:role/PersonalizeRoleRanking\n"
     ]
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeRoleRanking\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = policy_arn\n",
    ")\n",
    "\n",
    "# Now add S3 support\n",
    "iam.attach_role_policy(\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    RoleName=role_name\n",
    ")\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the interactions data <a class=\"anchor\" id=\"import\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Earlier you created the dataset group and dataset to house your information, so now you will execute an import job that will load the data from the S3 bucket into the Amazon Personalize dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:144386903708:dataset-import-job/personalize-ranking-example\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"7eab862d-9724-41dc-87d3-7455b4f16ead\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 20:51:57 GMT\",\n",
      "      \"x-amzn-requestid\": \"7eab862d-9724-41dc-87d3-7455b4f16ead\",\n",
      "      \"content-length\": \"115\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-ranking-example\",\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the dataset, the import job must be active. Execute the cell below and wait for it to show the ACTIVE status. It checks the status of the import job every second, up to a maximum of 3 hours.\n",
    "\n",
    "Importing the data can take some time, depending on the size of the dataset. In this workshop, the data import job should take around 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: CREATE IN_PROGRESS\n",
      "DatasetImportJob: ACTIVE\n",
      "CPU times: user 50 ms, sys: 15.1 ms, total: 65 ms\n",
      "Wall time: 13min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset import is active, you are ready to start building models with SIMS, Personalized-Ranking, Popularity-Count, and HRNN. This process will continue in other notebooks. Run the cell below before moving on to store a few values for usage in the next notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create solutions <a class=\"anchor\" id=\"solutions\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "In this notebook, you will create solutions with the following recipe:\n",
    "\n",
    "1. HRNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Amazon Personalize, a specific variation of an algorithm is called a recipe. Different recipes are suitable for different situations. A trained model is called a solution, and each solution can have many versions that relate to a given volume of data when the model was trained.\n",
    "\n",
    "To start, we will list all the recipes that are supported. This will allow you to select one and use that to build your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipes': [{'name': 'aws-hrnn',\n",
       "   'recipeArn': 'arn:aws:personalize:::recipe/aws-hrnn',\n",
       "   'status': 'ACTIVE',\n",
       "   'creationDateTime': datetime.datetime(2019, 6, 10, 0, 0, tzinfo=tzlocal()),\n",
       "   'lastUpdatedDateTime': datetime.datetime(2020, 5, 22, 6, 20, 26, 250000, tzinfo=tzlocal())},\n",
       "  {'name': 'aws-hrnn-coldstart',\n",
       "   'recipeArn': 'arn:aws:personalize:::recipe/aws-hrnn-coldstart',\n",
       "   'status': 'ACTIVE',\n",
       "   'creationDateTime': datetime.datetime(2019, 6, 10, 0, 0, tzinfo=tzlocal()),\n",
       "   'lastUpdatedDateTime': datetime.datetime(2020, 5, 22, 6, 20, 26, 250000, tzinfo=tzlocal())},\n",
       "  {'name': 'aws-hrnn-metadata',\n",
       "   'recipeArn': 'arn:aws:personalize:::recipe/aws-hrnn-metadata',\n",
       "   'status': 'ACTIVE',\n",
       "   'creationDateTime': datetime.datetime(2019, 6, 10, 0, 0, tzinfo=tzlocal()),\n",
       "   'lastUpdatedDateTime': datetime.datetime(2020, 5, 22, 6, 20, 26, 250000, tzinfo=tzlocal())},\n",
       "  {'name': 'aws-personalized-ranking',\n",
       "   'recipeArn': 'arn:aws:personalize:::recipe/aws-personalized-ranking',\n",
       "   'status': 'ACTIVE',\n",
       "   'creationDateTime': datetime.datetime(2019, 6, 10, 0, 0, tzinfo=tzlocal()),\n",
       "   'lastUpdatedDateTime': datetime.datetime(2020, 5, 22, 6, 20, 26, 250000, tzinfo=tzlocal())},\n",
       "  {'name': 'aws-popularity-count',\n",
       "   'recipeArn': 'arn:aws:personalize:::recipe/aws-popularity-count',\n",
       "   'status': 'ACTIVE',\n",
       "   'creationDateTime': datetime.datetime(2019, 6, 10, 0, 0, tzinfo=tzlocal()),\n",
       "   'lastUpdatedDateTime': datetime.datetime(2020, 5, 22, 6, 20, 26, 250000, tzinfo=tzlocal())},\n",
       "  {'name': 'aws-sims',\n",
       "   'recipeArn': 'arn:aws:personalize:::recipe/aws-sims',\n",
       "   'status': 'ACTIVE',\n",
       "   'creationDateTime': datetime.datetime(2019, 6, 10, 0, 0, tzinfo=tzlocal()),\n",
       "   'lastUpdatedDateTime': datetime.datetime(2020, 5, 22, 6, 20, 26, 250000, tzinfo=tzlocal())}],\n",
       " 'ResponseMetadata': {'RequestId': '4717776d-5577-4787-aab4-f74ee0148dcd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Tue, 26 May 2020 21:04:58 GMT',\n",
       "   'x-amzn-requestid': '4717776d-5577-4787-aab4-f74ee0148dcd',\n",
       "   'content-length': '1061',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personalize.list_recipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized Ranking\n",
    "\n",
    "Personalized Ranking is an interesting application of HRNN. Instead of just recommending what is most probable for the user in question, this algorithm takes in a user and a list of items as well. The items are then rendered back in the order of most probable relevance for the user. The use case here is for filtering on genre for example, or when you have a broad collection that you would like better ordered for a particular user.\n",
    "\n",
    "For our use case, using the LastFM data, we could imagine that a particular record label is paying us to recommend their artists to our users in a special promotion. Therefore, we know the list of artists we want to recommend, but we want to find out which of these artists each user will like most. We would use personalized ranking to re-order the list of artists for each user, based on their previous tagging history. \n",
    "\n",
    "Just like last time, we start by selecting the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_recipe_arn = \"arn:aws:personalize:::recipe/aws-personalized-ranking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the solution\n",
    "\n",
    "As with the previous solution, start by creating the solution first. Although you provide the dataset ARN in this step, the model is not yet trained. See this as an identifier instead of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:144386903708:solution/personalize-ranking\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"ba5cdcc0-de91-4bf0-a06d-dde91ff1b6ac\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 21:04:58 GMT\",\n",
      "      \"x-amzn-requestid\": \"ba5cdcc0-de91-4bf0-a06d-dde91ff1b6ac\",\n",
      "      \"content-length\": \"89\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rerank_create_solution_response = personalize.create_solution(\n",
    "    name = \"personalize-ranking\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = rerank_recipe_arn\n",
    ")\n",
    "\n",
    "rerank_solution_arn = rerank_create_solution_response['solutionArn']\n",
    "print(json.dumps(rerank_create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the solution version\n",
    "\n",
    "Once you have a solution, you need to create a version in order to complete the model training. The training can take a while to complete, upwards of 25 minutes, and an average of 35 minutes for this recipe with our dataset. Normally, we would use a while loop to poll until the task is completed. However the task would block other cells from executing, and the goal here is to create many models and deploy them quickly. So we will set up the while loop for all of the solutions further down in the notebook. There, you will also find instructions for viewing the progress in the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_create_solution_version_response = personalize.create_solution_version(\n",
    "    solutionArn = rerank_solution_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionVersionArn\": \"arn:aws:personalize:us-east-1:144386903708:solution/personalize-ranking/0f00a819\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"b07c347c-48aa-4650-86f6-53197351a62f\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 21:04:58 GMT\",\n",
      "      \"x-amzn-requestid\": \"b07c347c-48aa-4650-86f6-53197351a62f\",\n",
      "      \"content-length\": \"105\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rerank_solution_version_arn = rerank_create_solution_version_response['solutionVersionArn']\n",
    "print(json.dumps(rerank_create_solution_version_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View solution creation status\n",
    "\n",
    "As promised, how to view the status updates in the console:\n",
    "\n",
    "* In another browser tab you should already have the AWS Console up from opening this notebook instance. \n",
    "* Switch to that tab and search at the top for the service `Personalize`, then go to that service page. \n",
    "* Click `View dataset groups`.\n",
    "* Click the name of your dataset group, most likely something with POC in the name.\n",
    "* Click `Solutions and recipes`.\n",
    "* You will now see a list of all of the solutions you created above,  including a column with the status of the solution versions. Once it is `Active`, your solution is ready to be reviewed. It is also capable of being deployed.\n",
    "\n",
    "Or simply run the cell below to keep track of the solution version creation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build succeeded for arn:aws:personalize:us-east-1:144386903708:solution/personalize-ranking/0f00a819\n"
     ]
    }
   ],
   "source": [
    "in_progress_solution_versions = [\n",
    "    rerank_solution_version_arn\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for solution_version_arn in in_progress_solution_versions:\n",
    "        version_response = personalize.describe_solution_version(\n",
    "            solutionVersionArn = solution_version_arn\n",
    "        )\n",
    "        status = version_response[\"solutionVersion\"][\"status\"]\n",
    "        \n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "    \n",
    "    if len(in_progress_solution_versions) <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"At least one solution build is still in progress\")\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create campaigns <a class=\"anchor\" id=\"create\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "A campaign is a hosted solution version; an endpoint which you can query for recommendations. Pricing is set by estimating throughput capacity (requests from users for personalization per second). When deploying a campaign, you set a minimum throughput per second (TPS) value. This service, like many within AWS, will automatically scale based on demand, but if latency is critical, you may want to provision ahead for larger demand. For this POC and demo, all minimum throughput thresholds are set to 1. For more information, see the [pricing page](https://aws.amazon.com/personalize/pricing/).\n",
    "\n",
    "Let's start deploying the campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized Ranking\n",
    "\n",
    "Deploy a campaign for your personalized ranking solution version. It can take around 10 minutes to deploy a campaign. Normally, we would use a while loop to poll until the task is completed. However the task would block other cells from executing, and the goal here is to create multiple campaigns. So we will set up the while loop for all of the campaigns further down in the notebook. There, you will also find instructions for viewing the progress in the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"campaignArn\": \"arn:aws:personalize:us-east-1:144386903708:campaign/personalize-poc-rerank\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"61fd83dd-e409-4433-a2b9-667aecdecb0a\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Tue, 26 May 2020 21:48:55 GMT\",\n",
      "      \"x-amzn-requestid\": \"61fd83dd-e409-4433-a2b9-667aecdecb0a\",\n",
      "      \"content-length\": \"92\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rerank_create_campaign_response = personalize.create_campaign(\n",
    "    name = \"personalize-poc-rerank\",\n",
    "    solutionVersionArn = rerank_solution_version_arn,\n",
    "    minProvisionedTPS = 1\n",
    ")\n",
    "\n",
    "rerank_campaign_arn = rerank_create_campaign_response['campaignArn']\n",
    "print(json.dumps(rerank_create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View campaign creation status\n",
    "\n",
    "As promised, how to view the status updates in the console:\n",
    "\n",
    "* In another browser tab you should already have the AWS Console up from opening this notebook instance. \n",
    "* Switch to that tab and search at the top for the service `Personalize`, then go to that service page. \n",
    "* Click `View dataset groups`.\n",
    "* Click the name of your dataset group, most likely something with POC in the name.\n",
    "* Click `Campaigns`.\n",
    "* You will now see a list of all of the campaigns you created above, including a column with the status of the campaign. Once it is `Active`, your campaign is ready to be queried.\n",
    "\n",
    "Or simply run the cell below to keep track of the campaign creation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "At least one campaign build is still in progress\n",
      "Build succeeded for arn:aws:personalize:us-east-1:144386903708:campaign/personalize-poc-rerank\n"
     ]
    }
   ],
   "source": [
    "in_progress_campaigns = [\n",
    "    rerank_campaign_arn\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for campaign_arn in in_progress_campaigns:\n",
    "        version_response = personalize.describe_campaign(\n",
    "            campaignArn = campaign_arn\n",
    "        )\n",
    "        status = version_response[\"campaign\"][\"status\"]\n",
    "        \n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(campaign_arn))\n",
    "            in_progress_campaigns.remove(campaign_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(campaign_arn))\n",
    "            in_progress_campaigns.remove(campaign_arn)\n",
    "    \n",
    "    if len(in_progress_campaigns) <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"At least one campaign build is still in progress\")\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with campaigns <a class=\"anchor\" id=\"interact\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that all campaigns are deployed and active, we can start to get recommendations via an API call. Each of the campaigns is based on a different recipe, which behave in slightly different ways because they serve different use cases. We will cover each campaign in a different order than used in previous notebooks, in order to deal with the possible complexities in ascending order (i.e. simplest first).\n",
    "\n",
    "First, let's create a supporting function to help make sense of the results returned by a Personalize campaign. Personalize returns only an `item_id`. This is great for keeping data compact, but it means you need to query a database or lookup table to get a human-readable result for the notebooks. We will create a helper function to return a human-readable result from the LastFM dataset.\n",
    "\n",
    "Start by loading in the dataset which we can use for our lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>pictureURL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MALICE MIZER</td>\n",
       "      <td>http://www.last.fm/music/MALICE+MIZER</td>\n",
       "      <td>http://userserve-ak.last.fm/serve/252/10808.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Diary of Dreams</td>\n",
       "      <td>http://www.last.fm/music/Diary+of+Dreams</td>\n",
       "      <td>http://userserve-ak.last.fm/serve/252/3052066.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carpathian Forest</td>\n",
       "      <td>http://www.last.fm/music/Carpathian+Forest</td>\n",
       "      <td>http://userserve-ak.last.fm/serve/252/40222717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moi dix Mois</td>\n",
       "      <td>http://www.last.fm/music/Moi+dix+Mois</td>\n",
       "      <td>http://userserve-ak.last.fm/serve/252/54697835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bella Morte</td>\n",
       "      <td>http://www.last.fm/music/Bella+Morte</td>\n",
       "      <td>http://userserve-ak.last.fm/serve/252/14789013...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                         url  \\\n",
       "id                                                                  \n",
       "1        MALICE MIZER       http://www.last.fm/music/MALICE+MIZER   \n",
       "2     Diary of Dreams    http://www.last.fm/music/Diary+of+Dreams   \n",
       "3   Carpathian Forest  http://www.last.fm/music/Carpathian+Forest   \n",
       "4        Moi dix Mois       http://www.last.fm/music/Moi+dix+Mois   \n",
       "5         Bella Morte        http://www.last.fm/music/Bella+Morte   \n",
       "\n",
       "                                           pictureURL  \n",
       "id                                                     \n",
       "1     http://userserve-ak.last.fm/serve/252/10808.jpg  \n",
       "2   http://userserve-ak.last.fm/serve/252/3052066.jpg  \n",
       "3   http://userserve-ak.last.fm/serve/252/40222717...  \n",
       "4   http://userserve-ak.last.fm/serve/252/54697835...  \n",
       "5   http://userserve-ak.last.fm/serve/252/14789013...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe for the items by reading in the correct source CSV\n",
    "items_df = pd.read_csv(data_dir + '/artists.dat', delimiter='\\t', index_col=0)\n",
    "\n",
    "# Render some sample data\n",
    "items_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining the ID column as the index column it is trivial to return an artist by just querying the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth, Wind & Fire\n"
     ]
    }
   ],
   "source": [
    "item_id_example = 987\n",
    "artist = items_df.loc[item_id_example]['name']\n",
    "print(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That isn't terrible, but it would get messy to repeat this everywhere in our code, so the function below will clean that up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_by_id(artist_id, artist_df=items_df):\n",
    "    \"\"\"\n",
    "    This takes in an artist_id from Personalize so it will be a string,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return artist_df.loc[int(artist_id)]['name']\n",
    "    except:\n",
    "        return \"Error obtaining artist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test a few simple values to check our error catching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth, Wind & Fire\n",
      "Error obtaining artist\n",
      "Error obtaining artist\n"
     ]
    }
   ],
   "source": [
    "# A known good id\n",
    "print(get_artist_by_id(artist_id=\"987\"))\n",
    "# A bad type of value\n",
    "print(get_artist_by_id(artist_id=\"987.9393939\"))\n",
    "# Really bad values\n",
    "print(get_artist_by_id(artist_id=\"Steve\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a way of rendering results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>13883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>11690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>11351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>8983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artistID  weight\n",
       "userID                  \n",
       "2             51   13883\n",
       "2             52   11690\n",
       "2             53   11351\n",
       "2             54   10300\n",
       "2             55    8983"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df = pd.read_csv(data_dir + '/user_artists.dat', delimiter='\\t', index_col=0)\n",
    "# Render some sample data\n",
    "users_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized Ranking\n",
    "\n",
    "The core use case for personalized ranking is to take a collection of items and to render them in priority or probable order of interest for a user. To demonstrate this, we will need a random user and a random collection of 25 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_user = users_df.sample(1).index.tolist()[0]\n",
    "rerank_items = items_df.sample(25).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a nice dataframe that shows the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1303</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Priestess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пелагея</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cosmic Gate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mujuice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WIZO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Love of Lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mayhem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Taking Dawn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Universum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ahmet Kaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tomate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chris Willis; David Guetta; Fergie; LMFAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Farid Farjad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>McFly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How to Destroy Angels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inside the Whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bellize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>New Found Glory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Audrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tobacco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Murder by Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>múm &amp; Úrú Búrú Sextettinn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         1303\n",
       "0                                   Priestess\n",
       "1                                     Пелагея\n",
       "2                                 Cosmic Gate\n",
       "3                                     Mujuice\n",
       "4                                       Prism\n",
       "5                                        WIZO\n",
       "6                             Love of Lesbian\n",
       "7                                      Mayhem\n",
       "8                                 Taking Dawn\n",
       "9                                   Universum\n",
       "10                                 Ahmet Kaya\n",
       "11                                     Tomate\n",
       "12  Chris Willis; David Guetta; Fergie; LMFAO\n",
       "13                               Farid Farjad\n",
       "14                                      McFly\n",
       "15                      How to Destroy Angels\n",
       "16                           Inside the Whale\n",
       "17                                    Bellize\n",
       "18                               The Business\n",
       "19                                  The Title\n",
       "20                            New Found Glory\n",
       "21                                     Audrey\n",
       "22                                    Tobacco\n",
       "23                            Murder by Death\n",
       "24                  múm & Úrú Búrú Sextettinn"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_list = []\n",
    "for item in rerank_items:\n",
    "    artist = get_artist_by_id(item)\n",
    "    rerank_list.append(artist)\n",
    "rerank_df = pd.DataFrame(rerank_list, columns = [rerank_user])\n",
    "rerank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make the personalized ranking API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'fd2ea40f-11f5-461c-b678-6a6a08bdb416',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/json',\n",
       "   'date': 'Tue, 26 May 2020 22:02:55 GMT',\n",
       "   'x-amzn-requestid': 'fd2ea40f-11f5-461c-b678-6a6a08bdb416',\n",
       "   'content-length': '1442',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0},\n",
       " 'personalizedRanking': [{'itemId': '1047', 'score': 0.1730128},\n",
       "  {'itemId': '1046', 'score': 0.1639514},\n",
       "  {'itemId': '1254', 'score': 0.1101557},\n",
       "  {'itemId': '18692', 'score': 0.0762716},\n",
       "  {'itemId': '5783', 'score': 0.0678702},\n",
       "  {'itemId': '13434', 'score': 0.067667},\n",
       "  {'itemId': '8915', 'score': 0.0616803},\n",
       "  {'itemId': '1305', 'score': 0.0322442},\n",
       "  {'itemId': '8543', 'score': 0.0297265},\n",
       "  {'itemId': '11979', 'score': 0.0286533},\n",
       "  {'itemId': '6820', 'score': 0.025196},\n",
       "  {'itemId': '4154', 'score': 0.0246835},\n",
       "  {'itemId': '2312', 'score': 0.0228279},\n",
       "  {'itemId': '2158', 'score': 0.0179717},\n",
       "  {'itemId': '2036', 'score': 0.0176177},\n",
       "  {'itemId': '17786', 'score': 0.0166342},\n",
       "  {'itemId': '12605', 'score': 0.0164588},\n",
       "  {'itemId': '5671', 'score': 0.0163159},\n",
       "  {'itemId': '8834', 'score': 0.0161517},\n",
       "  {'itemId': '12025', 'score': 0.0149095},\n",
       "  {'itemId': '3387'},\n",
       "  {'itemId': '14552'},\n",
       "  {'itemId': '6359'},\n",
       "  {'itemId': '6609'},\n",
       "  {'itemId': '9998'}],\n",
       " 'recommendationId': 'RID-279bc670-9f3a-4633-b670-1183fd50c323'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert user to string:\n",
    "user_id = str(rerank_user)\n",
    "rerank_item_list = []\n",
    "for item in rerank_items:\n",
    "    rerank_item_list.append(str(item))\n",
    "    \n",
    "# Get recommended reranking\n",
    "get_recommendations_response_rerank = personalize_runtime.get_personalized_ranking(\n",
    "        campaignArn = rerank_campaign_arn,\n",
    "        userId = user_id,\n",
    "        inputList = rerank_item_list\n",
    ")\n",
    "\n",
    "get_recommendations_response_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the reranked items as a second column to the original dataframe, for a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1303</th>\n",
       "      <th>Re-Ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Priestess</td>\n",
       "      <td>McFly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пелагея</td>\n",
       "      <td>New Found Glory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cosmic Gate</td>\n",
       "      <td>Mayhem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mujuice</td>\n",
       "      <td>Farid Farjad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prism</td>\n",
       "      <td>Priestess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WIZO</td>\n",
       "      <td>Пелагея</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Love of Lesbian</td>\n",
       "      <td>How to Destroy Angels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mayhem</td>\n",
       "      <td>Cosmic Gate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Taking Dawn</td>\n",
       "      <td>Prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Universum</td>\n",
       "      <td>Murder by Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ahmet Kaya</td>\n",
       "      <td>Universum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tomate</td>\n",
       "      <td>WIZO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chris Willis; David Guetta; Fergie; LMFAO</td>\n",
       "      <td>Ahmet Kaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Farid Farjad</td>\n",
       "      <td>Mujuice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>McFly</td>\n",
       "      <td>The Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How to Destroy Angels</td>\n",
       "      <td>Bellize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inside the Whale</td>\n",
       "      <td>Tobacco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bellize</td>\n",
       "      <td>Taking Dawn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Business</td>\n",
       "      <td>Love of Lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Title</td>\n",
       "      <td>Audrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>New Found Glory</td>\n",
       "      <td>Tomate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Audrey</td>\n",
       "      <td>Chris Willis; David Guetta; Fergie; LMFAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tobacco</td>\n",
       "      <td>Inside the Whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Murder by Death</td>\n",
       "      <td>The Title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>múm &amp; Úrú Búrú Sextettinn</td>\n",
       "      <td>múm &amp; Úrú Búrú Sextettinn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         1303  \\\n",
       "0                                   Priestess   \n",
       "1                                     Пелагея   \n",
       "2                                 Cosmic Gate   \n",
       "3                                     Mujuice   \n",
       "4                                       Prism   \n",
       "5                                        WIZO   \n",
       "6                             Love of Lesbian   \n",
       "7                                      Mayhem   \n",
       "8                                 Taking Dawn   \n",
       "9                                   Universum   \n",
       "10                                 Ahmet Kaya   \n",
       "11                                     Tomate   \n",
       "12  Chris Willis; David Guetta; Fergie; LMFAO   \n",
       "13                               Farid Farjad   \n",
       "14                                      McFly   \n",
       "15                      How to Destroy Angels   \n",
       "16                           Inside the Whale   \n",
       "17                                    Bellize   \n",
       "18                               The Business   \n",
       "19                                  The Title   \n",
       "20                            New Found Glory   \n",
       "21                                     Audrey   \n",
       "22                                    Tobacco   \n",
       "23                            Murder by Death   \n",
       "24                  múm & Úrú Búrú Sextettinn   \n",
       "\n",
       "                                    Re-Ranked  \n",
       "0                                       McFly  \n",
       "1                             New Found Glory  \n",
       "2                                      Mayhem  \n",
       "3                                Farid Farjad  \n",
       "4                                   Priestess  \n",
       "5                                     Пелагея  \n",
       "6                       How to Destroy Angels  \n",
       "7                                 Cosmic Gate  \n",
       "8                                       Prism  \n",
       "9                             Murder by Death  \n",
       "10                                  Universum  \n",
       "11                                       WIZO  \n",
       "12                                 Ahmet Kaya  \n",
       "13                                    Mujuice  \n",
       "14                               The Business  \n",
       "15                                    Bellize  \n",
       "16                                    Tobacco  \n",
       "17                                Taking Dawn  \n",
       "18                            Love of Lesbian  \n",
       "19                                     Audrey  \n",
       "20                                     Tomate  \n",
       "21  Chris Willis; David Guetta; Fergie; LMFAO  \n",
       "22                           Inside the Whale  \n",
       "23                                  The Title  \n",
       "24                  múm & Úrú Búrú Sextettinn  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_list = []\n",
    "item_list = get_recommendations_response_rerank['personalizedRanking']\n",
    "for item in item_list:\n",
    "    artist = get_artist_by_id(item['itemId'])\n",
    "    ranked_list.append(artist)\n",
    "ranked_df = pd.DataFrame(ranked_list, columns = ['Re-Ranked'])\n",
    "rerank_df = pd.concat([rerank_df, ranked_df], axis=1)\n",
    "rerank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above how each entry was re-ordered based on the model's understanding of the user. This is a popular task when you have a collection of items to surface a user, a list of promotions for example, or if you are filtering on a category and want to show the most likely good items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up campaigns\n",
    "\n",
    "Start by cleaning up the campaigns. You need to delete campaigns before you can delete the solution versions upon which they are based. \n",
    "\n",
    "The code below will list all of the campaigns in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_campaigns')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for campaign in paginate_result[\"campaigns\"]:\n",
    "        print(campaign[\"campaignArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through the list of ARNs to determine which campaigns you want to delete. Then use the code below to delete the campaign by inserting the ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_campaign(\n",
    "    campaignArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up solutions\n",
    "\n",
    "Next, clean up the solutions. The code below will list all of the solutions in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_solutions')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for solution in paginate_result[\"solutions\"]:\n",
    "        print(solution[\"solutionArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through the list of ARNs to determine which solution you want to delete. Then use the code below to delete the it by inserting the ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_solution(\n",
    "    solutionArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up datasets\n",
    "\n",
    "Next, clean up the datasets. The code below will list all of the datasets in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_datasets')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for datasets in paginate_result[\"datasets\"]:\n",
    "        print(datasets[\"datasetArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through the list of ARNs to determine which dataset you want to delete. Then use the code below to delete the it by inserting the ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_dataset(\n",
    "    datasetArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the schemas\n",
    "\n",
    "Next, clean up the schemas. The code below will list all of the schemas in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_schemas')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for schema in paginate_result[\"schemas\"]:\n",
    "        print(schema[\"schemaArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through the list of ARNs to determine which schema you want to delete. Then use the code below to delete the it by inserting the ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_schema(\n",
    "    schemaArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the dataset groups\n",
    "\n",
    "Finally, clean up the dataset group. The code below will list all of the dataset groups in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = personalize.get_paginator('list_dataset_groups')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for dataset_group in paginate_result[\"datasetGroups\"]:\n",
    "        print(dataset_group[\"datasetGroupArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through the list of ARNs to determine which dataset group you want to delete. Then use the code below to delete the it by inserting the ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.delete_dataset_group(\n",
    "    datasetGroupArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the S3 bucket and IAM role\n",
    "\n",
    "Optionally, you can delete the IAM role and the S3 bucket which we created throughout the workshop.\n",
    "\n",
    "Start by listing all of the IAM roles in your account with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam')\n",
    "\n",
    "paginator = iam.get_paginator('list_roles')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for roles in paginate_result[\"Roles\"]:\n",
    "        print(roles[\"RoleName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the name of the role you want to delete.\n",
    "\n",
    "You cannot delete an IAM role which still has policies attached to it. So after you have identified the relevant role, let's list the attached policies of that role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.list_attached_role_policies(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to detach the policies in the result above using the code below. Repeat for each attached policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\",\n",
    "    PolicyArn = \"INSERT ARN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you should be able to delete the IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_role(\n",
    "    RoleName = \"INSERT ROLE NAME HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete an S3 bucket, it first needs to be empty. The easiest way to delete an S3 bucket, is just to navigate to S3 in the AWS console, delete the objects in the bucket, and then delete the S3 bucket itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
